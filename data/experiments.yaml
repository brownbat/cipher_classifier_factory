- data_params:
    ciphers:
    - english
    - caesar
    - vigenere
    - columnar_transposition
    num_samples: 10000
    sample_length: 500
  experiment_id: exp_1_256_hidden_20_layers
  hyperparams:
    batch_size: 32
    dropout_rate: 0.015
    embedding_dim: 128
    epochs: 15
    hidden_dim: 256
    learning_rate: 0.001
    num_layers: 20
  metrics:
    conf_matrix:
    - - - 109
        - 0
        - 0
        - 395
      - - 0
        - 428
        - 83
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 46
        - 0
        - 0
        - 447
    - - - 186
        - 0
        - 0
        - 318
      - - 0
        - 428
        - 83
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 57
        - 0
        - 0
        - 436
    - - - 417
        - 0
        - 0
        - 87
      - - 0
        - 427
        - 82
        - 2
      - - 0
        - 0
        - 489
        - 3
      - - 46
        - 0
        - 0
        - 447
    - - - 440
        - 0
        - 0
        - 64
      - - 0
        - 428
        - 83
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 24
        - 0
        - 0
        - 469
    - - - 465
        - 0
        - 0
        - 39
      - - 0
        - 428
        - 83
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 7
        - 0
        - 0
        - 486
    - - - 492
        - 0
        - 0
        - 12
      - - 0
        - 428
        - 83
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 21
        - 0
        - 0
        - 472
    - - - 500
        - 0
        - 0
        - 4
      - - 0
        - 428
        - 83
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 2
        - 0
        - 0
        - 491
    - - - 497
        - 0
        - 0
        - 7
      - - 0
        - 511
        - 0
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 1
        - 0
        - 0
        - 492
    - - - 476
        - 0
        - 0
        - 28
      - - 0
        - 511
        - 0
        - 0
      - - 0
        - 1
        - 491
        - 0
      - - 0
        - 1
        - 0
        - 492
    - - - 503
        - 0
        - 0
        - 1
      - - 0
        - 509
        - 2
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 4
        - 0
        - 0
        - 489
    - - - 504
        - 0
        - 0
        - 0
      - - 0
        - 511
        - 0
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 4
        - 0
        - 0
        - 489
    - - - 499
        - 0
        - 0
        - 5
      - - 0
        - 511
        - 0
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 0
        - 0
        - 0
        - 493
    - - - 502
        - 0
        - 0
        - 2
      - - 0
        - 511
        - 0
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 0
        - 0
        - 0
        - 493
    - - - 503
        - 0
        - 0
        - 1
      - - 0
        - 511
        - 0
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 4
        - 0
        - 0
        - 489
    - - - 503
        - 0
        - 0
        - 1
      - - 0
        - 511
        - 0
        - 0
      - - 0
        - 0
        - 492
        - 0
      - - 2
        - 0
        - 0
        - 491
    train_loss:
    - 0.7243980667591094
    - 0.4698297468423843
    - 0.41435504919290544
    - 0.3973815178573132
    - 0.2262125738710165
    - 0.22220034354925156
    - 0.16210567558556796
    - 0.06640615915972739
    - 0.012934596850769595
    - 0.014481289681047201
    - 0.01074611401371658
    - 0.014812766478280536
    - 0.006971618282492272
    - 0.006898608854040504
    - 0.00034193626741762275
    training_time: 960.0145533084869
    val_accuracy:
    - 0.738
    - 0.771
    - 0.89
    - 0.9145
    - 0.9355
    - 0.942
    - 0.9555
    - 0.996
    - 0.985
    - 0.9965
    - 0.998
    - 0.9975
    - 0.999
    - 0.9975
    - 0.9985
    val_loss:
    - 0.46913394757679533
    - 0.4720085400437552
    - 0.31573436456540277
    - 0.26698303080740426
    - 0.19637802932115775
    - 0.19433465736016395
    - 0.13779753922588295
    - 0.017573363953343933
    - 0.04513545658823753
    - 0.01536285954283639
    - 0.006755379613466738
    - 0.007997520054997286
    - 0.004443698892987053
    - 0.0035115106997730596
    - 0.0028012982123376182
- data_params:
    ciphers:
    - english
    - caesar
    - vigenere
    - columnar_transposition
    num_samples: 10000
    sample_length: 500
  experiment_id: exp_2_128_hidden_30_layers
  hyperparams:
    batch_size: 32
    dropout_rate: 0.015
    embedding_dim: 128
    epochs: 15
    hidden_dim: 128
    learning_rate: 0.001
    num_layers: 30
  metrics:
    conf_matrix:
    - - - 524
        - 0
        - 0
        - 0
      - - 0
        - 397
        - 82
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 493
        - 0
        - 1
        - 4
    - - - 522
        - 0
        - 0
        - 2
      - - 0
        - 397
        - 82
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 492
        - 0
        - 3
        - 3
    - - - 301
        - 0
        - 0
        - 223
      - - 0
        - 397
        - 82
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 220
        - 0
        - 0
        - 278
    - - - 266
        - 0
        - 0
        - 258
      - - 0
        - 397
        - 82
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 51
        - 0
        - 0
        - 447
    - - - 242
        - 0
        - 0
        - 282
      - - 0
        - 397
        - 82
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 99
        - 0
        - 0
        - 399
    - - - 264
        - 0
        - 0
        - 260
      - - 0
        - 397
        - 82
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 42
        - 0
        - 0
        - 456
    - - - 321
        - 0
        - 0
        - 203
      - - 0
        - 397
        - 82
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 42
        - 0
        - 0
        - 456
    - - - 428
        - 0
        - 0
        - 96
      - - 0
        - 400
        - 79
        - 0
      - - 0
        - 5
        - 494
        - 0
      - - 39
        - 0
        - 0
        - 459
    - - - 433
        - 0
        - 0
        - 91
      - - 0
        - 396
        - 82
        - 1
      - - 0
        - 0
        - 497
        - 2
      - - 15
        - 0
        - 0
        - 483
    - - - 470
        - 0
        - 0
        - 54
      - - 0
        - 454
        - 25
        - 0
      - - 0
        - 50
        - 449
        - 0
      - - 18
        - 0
        - 0
        - 480
    - - - 467
        - 0
        - 0
        - 57
      - - 0
        - 475
        - 4
        - 0
      - - 0
        - 1
        - 498
        - 0
      - - 13
        - 0
        - 0
        - 485
    - - - 466
        - 0
        - 0
        - 58
      - - 0
        - 479
        - 0
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 19
        - 0
        - 0
        - 479
    - - - 466
        - 0
        - 0
        - 58
      - - 0
        - 479
        - 0
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 9
        - 0
        - 0
        - 489
    - - - 479
        - 0
        - 0
        - 45
      - - 0
        - 479
        - 0
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 9
        - 0
        - 0
        - 489
    - - - 457
        - 0
        - 0
        - 67
      - - 0
        - 479
        - 0
        - 0
      - - 0
        - 0
        - 499
        - 0
      - - 4
        - 0
        - 0
        - 494
    train_loss:
    - 0.7088791699409485
    - 0.5663229129314422
    - 0.48867614030838014
    - 0.4750687246322632
    - 0.4902892523407936
    - 0.5033816487193108
    - 0.4060551614165306
    - 0.3345181553065777
    - 0.2665473595559597
    - 0.2445574924647808
    - 0.17134307761490344
    - 0.09909501477330923
    - 0.07811007266491651
    - 0.0727332192696631
    - 0.0680235688611865
    training_time: 399.3348915576935
    val_accuracy:
    - 0.712
    - 0.7105
    - 0.7375
    - 0.8045
    - 0.7685
    - 0.808
    - 0.8365
    - 0.8905
    - 0.9045
    - 0.9265
    - 0.9625
    - 0.9615
    - 0.9665
    - 0.973
    - 0.9645
    val_loss:
    - 0.4852224380250961
    - 0.48970761469432283
    - 0.4742509907200223
    - 0.44133935892392717
    - 0.4976302749580807
    - 0.43153179259527297
    - 0.38595903889527394
    - 0.29921655853589374
    - 0.2753592650332148
    - 0.22667816435060803
    - 0.13707514532974788
    - 0.1246430135465094
    - 0.1053176261484623
    - 0.09061920342759953
    - 0.12722139557941803
